{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b69684e",
   "metadata": {},
   "source": [
    "# Introducción a la Ciencia de Datos: Tarea 2\n",
    "\n",
    "Este notebook contiene el código de base para realizar la Tarea 2 del curso. Puede copiarlo en su propio repositorio y trabajar sobre el mismo.\n",
    "Las **instrucciones para ejecutar el notebook** están en la [página inicial del repositorio](https://gitlab.fing.edu.uy/maestria-cdaa/intro-cd/).\n",
    "\n",
    "**Se espera que no sea necesario revisar el código para corregir la tarea**, ya que todos los resultados y análisis relevantes deberían estar en el **informe en formato PDF**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38d94a",
   "metadata": {},
   "source": [
    "## Cargar bibliotecas (dependencias)\n",
    "Recuerde instalar los requerimientos (`requirements.txt`) en el mismo entorno donde está ejecutando este notebook (ver [README](https://gitlab.fing.edu.uy/maestria-cdaa/intro-cd/)). Para la entrega 2 hay nuevas dependencias, por lo que es importante correr la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7abc091c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try brew install\n",
      "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m use a virtual environment:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
      "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
      "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m brew install pipx\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
      "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[31m   \u001b[0m 'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[31m   \u001b[0m will permanently disable this error.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try brew install\n",
      "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m use a virtual environment:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
      "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
      "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m brew install pipx\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
      "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[31m   \u001b[0m 'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[31m   \u001b[0m will permanently disable this error.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "/opt/homebrew/opt/python@3.13/bin/python3.13: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt\n",
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644680d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')  # Necesario para tokenizar\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')   # Para sinónimos y definiciones\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # Para etiquetas gramaticales\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e5616",
   "metadata": {},
   "source": [
    "## Lectura de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7227c346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DataFrame con todos los discursos:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#df_speeches = pd.read_csv('C:/Users/lulag/introCD/data/us_2020_election_speeches.csv')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df_speeches \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/us_2020_election_speeches.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# DataFrame con todos los discursos:\n",
    "pd.set_option('display.max_rows', None)\n",
    "#df_speeches = pd.read_csv('C:/Users/lulag/introCD/data/us_2020_election_speeches.csv')\n",
    "df_speeches = pd.read_csv('../data/us_2020_election_speeches.csv')\n",
    "df_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a82686f-e923-4429-8447-200389e2630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de speakers múltiples\n",
    "df_speeches['speaker'] = df_speeches['speaker'].str.split(',')\n",
    "df_speeches = df_speeches.explode('speaker')\n",
    "df_speeches['speaker'] = df_speeches['speaker'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6afed-e1d4-4175-9f63-244e6d85435a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selección de los 3 candidatos con más discursos\n",
    "presidents = [\"Joe Biden\", \"Donald Trump\", \"Mike Pence\"]\n",
    "\n",
    "# df_speeches_top_3 = ...\n",
    "def presencia(df, column, names):\n",
    "    return df[column].astype(str).apply(lambda x: 1 if any(name in x for name in names) else 0)\n",
    "boolean = presencia(df_speeches, 'speaker', presidents)    \n",
    "df_speeches_top_3 = df_speeches[boolean == 1]\n",
    "# Eliminación de Donald Trump Jr. (hijo)\n",
    "df_speeches_top_3 = df_speeches_top_3.drop(136)\n",
    "\n",
    "# Reseteo del índice\n",
    "df_speeches_top_3.reset_index(drop=True, inplace=True)\n",
    "df_speeches_top_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b429f-a784-43e8-aa69-f2c194d3667e",
   "metadata": {},
   "source": [
    "## Limpieza de Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "825a2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Función clean_text() de la entrega anterior\n",
    "\n",
    "def clean_text(df, column_name):\n",
    "    # Eliminar primeras palabras hasta el primer \"\\n\"\n",
    "    result = df[column_name].str.replace(r\"^[^\\n]*\\n\", \"\", regex=True)\n",
    "    # Convertir todo a minúsculas\n",
    "    result = result.str.lower()\n",
    "    # Completar signos de puntuación faltantes\n",
    "    for punc in [\"\\n\", \"[\", \"]\", \",\", \":\", \".\", \";\", \"!\", \"”\", \"“\", \"-\", \"/\", \"(\", \")\", \"?\",\"…\",\"’\",\"‘\"]:\n",
    "        result = result.str.replace(punc, \" \")\n",
    "    return result\n",
    "\n",
    "# Creación de una nueva columna CleanText a partir de text\n",
    "df_speeches_top_3[\"CleanText\"] = clean_text(df_speeches_top_3,'text')\n",
    "# df_speeches_top_3[\"CleanText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187f0674-97ba-4d47-9367-fb123a544162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 - Eliminación de stopwords\n",
    "\n",
    "# Lista de stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# print(stop_words)\n",
    "\n",
    "# Función para eliminar stopwords de un texto\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)  # Divide el texto en palabras\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "df_speeches_top_3[\"CleanText\"] = df_speeches_top_3[\"CleanText\"].apply(remove_stopwords)\n",
    "# df_speeches_top_3[\"CleanText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd23191-b244-4591-857a-fa9f6351849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Separación de contracciones y escritura de forma completa\n",
    "\n",
    "# La lista de stopwords que se eliminaron en la celda anterior incluye varias contracciones.\n",
    "# En este paso se expande la contracciones que puedan no haber sido consideradas por la lista de stopwords de nltk.\n",
    "contraction_list = {\n",
    "    \"let's\": \"let us\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"ain't\": \"is not\",\n",
    "}\n",
    "# Sustitución del apóstrofe gráfico\n",
    "df_speeches_top_3[\"CleanText\"] = df_speeches_top_3[\"CleanText\"].apply(lambda x: x.replace(\"’\", \"'\") if isinstance(x, str) else x)\n",
    "def expand_contractions(text, contractions=contraction_list):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in contractions.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contractions[x.group()], text)\n",
    "\n",
    "df_speeches_top_3[\"CleanText\"] = df_speeches_top_3[\"CleanText\"].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4caafc7d-a602-43b6-8269-e7a792bce1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Eliminación de números\n",
    "\n",
    "def remove_numbers(text):\n",
    "    # Elimina números, \n",
    "    # incluyendo sufijos como: 'th', 'st', 'nd', 'rd', 's'.\n",
    "    return re.sub(r'\\b\\d+(?:\\.\\d+)?(?:st|nd|rd|th|s|k|ks|census)?\\b', '', text)\n",
    "\n",
    "df_speeches_top_3[\"CleanText\"] = df_speeches_top_3[\"CleanText\"].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69585423-b1d2-425d-94f8-247adc08863e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5 - Lematización\n",
    "\n",
    "# Inicialización del lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Función para mapear etiquetas POS de nltk a las que requiere WordNet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default\n",
    "\n",
    "# Función de lematización\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "df_speeches_top_3[\"CleanText\"] = df_speeches_top_3[\"CleanText\"].apply(lemmatize_text)\n",
    "# df_speeches_top_3[\"CleanText\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7f004",
   "metadata": {},
   "source": [
    "## Parte 1: Dataset y representación numérica de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Separar del 30% del conjunto para test. Al 70 % restante para entrenamiento se lo llama \"dev\" (desarrollo).\n",
    "\n",
    "# Definición de variables\n",
    "X = df_speeches_top_3[\"CleanText\"]      # Características: transcripciones de discursos (columna 'CleanText' del DataFrame)\n",
    "y = df_speeches_top_3[\"speaker\"]        # Etiqueta: presidente que pronunció el discurso (columna 'speaker' del DataFrame)\n",
    "\n",
    "# División estratificada\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = 0.3,            # 30% se separa para test\n",
    "    stratify = y,               # Respeta las proporciones entre candidatos\n",
    "    random_state = 17           # Para reproducibilidad\n",
    ")\n",
    "\n",
    "print(f\"Tamaños de los conjuntos: \\nEntrenamiento:{X_dev.shape} Testeo:{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Visualización de la proporción de cada candidato por conjunto\n",
    "\n",
    "# Creación de un DataFrame con las proporciones\n",
    "train_dist = y_dev.value_counts(normalize=True).rename(\"Entrenamiento\")\n",
    "test_dist = y_test.value_counts(normalize=True).rename(\"Evaluación\")\n",
    "balance_df = pd.concat([train_dist, test_dist], axis=1)\n",
    "\n",
    "# Gráfico de barras\n",
    "balance_df.plot(kind='bar')\n",
    "plt.title(\"Proporción de discursos por candidato \\nen los conjuntos de entrenamiento y evaluación\")\n",
    "plt.xlabel(\"Candidato\")\n",
    "plt.ylabel(\"Proporción\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.legend(title='Conjunto:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab89198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Transforme el texto del conjunto de entrenamiento a la representación numérica (features) de conteo de palabras o bag of words.\n",
    "\n",
    "# Creación del vectorizador\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Entrenamiento y transformación de los textos\n",
    "X_dev_bow = vectorizer.fit_transform(X_dev)\n",
    "\n",
    "# Mostrar la forma de la matriz\n",
    "print(\"Dimensiones de la matriz BoW:\", X_dev_bow.shape)\n",
    "# Ejemplo: primeras 5 palabras del vocabulario\n",
    "print(\"Primeras palabras del vocabulario:\", vectorizer.get_feature_names_out()[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Obtenga la representación numérica Term Frequency - Inverse Document Frequency.\n",
    "\n",
    "# Inicialización del transformador\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# Ajuste y transformación de la matriz de conteo\n",
    "X_dev_tfidf = tfidf_transformer.fit_transform(X_dev_bow)\n",
    "\n",
    "# Mostrar la forma de la matriz\n",
    "print(\"Dimensiones de la matriz TfIdf:\", X_dev_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59b85f-6b44-4720-b642-1e46c1a92185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de TF-IDF > 0 para un discurso de ejemplo\n",
    "\n",
    "# Para el primer speech: Joe Biden\n",
    "text_index = 0\n",
    "\n",
    "# Vector TF-IDF en formato denso\n",
    "text_vector = X_dev_tfidf[text_index].toarray()[0]\n",
    "\n",
    "# Nombres de las palabras\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Filtro de términos con TF-IDF > 0\n",
    "non_zero_terms = [(feature_names[i], text_vector[i]) \n",
    "                  for i in range(len(text_vector)) if text_vector[i] > 0]\n",
    "\n",
    "# Visualización de términos con su peso (ordenados por importancia)\n",
    "non_zero_terms_sorted = sorted(non_zero_terms, key=lambda x: x[1], reverse=True)\n",
    "# Primeros 10\n",
    "print(non_zero_terms_sorted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ff24d-6976-4e20-aa0c-fe6839756006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de lo anterior como Wordmap\n",
    "\n",
    "# Creación del diccionario: {palabra: peso TF-IDF}\n",
    "word_weights = {\n",
    "    feature_names[i]: text_vector[i]\n",
    "    for i in range(len(text_vector)) if text_vector[i] > 0\n",
    "}\n",
    "\n",
    "# Generación de la nube de palabras\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "wordcloud.generate_from_frequencies(word_weights)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(f'Nube de palabras - Discurso #{text_index} {df_speeches_top_3.iloc[text_index][\"speaker\"]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31172c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 :Muestre en un mapa el conjunto de entrenamiento, utilizando las dos primeras componentes PCA sobre los vectores de tf-idf.\n",
    "\n",
    "# Aplicación de PCA \n",
    "components = 2\n",
    "pca = PCA(n_components = components)\n",
    "X_pca = pca.fit_transform(X_dev_tfidf.toarray())\n",
    "\n",
    "# Obtenención de etiquetas\n",
    "labels = y_dev.values\n",
    "\n",
    "# Scatter Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for speaker in set(labels):\n",
    "    idx = labels == speaker\n",
    "    plt.scatter(X_pca[idx, 0], X_pca[idx, 1], label=speaker, alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.title(\"Proyección PCA de los discursos (TF-IDF)\")\n",
    "plt.legend(title=\"Candidato:\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127de180-a3c1-4566-b010-cf5d260e1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Qué palabras contribuyen más al Componente Principal 1?\n",
    "\n",
    "# Obtenención de las componentes (matriz: n_componentes x n_palabras)\n",
    "components = pca.components_\n",
    "\n",
    "# Obtenención de nombres de las palabras del vectorizador\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Para la primera componente\n",
    "comp_idx = 0  \n",
    "top_indices = np.argsort(components[comp_idx])[::-1]  # Mayor contribución primero\n",
    "\n",
    "# Mostrar las 10 palabras que más contribuyen a la primera componente\n",
    "top_words = [(feature_names[i], components[comp_idx][i]) for i in top_indices[:10]]\n",
    "print(\"Palabras que más contribuyen a la componente principal 1:\")\n",
    "for word, weight in top_words:\n",
    "    print(f\"{word}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7085d6e-48e6-46df-9ba0-584cbe5c2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de palabras que más contribuyen al Componente Principal 1\n",
    "top_words = [(feature_names[i], components[comp_idx][i]) for i in top_indices[:100]]  # Se toma 100 para mejor visualización\n",
    "\n",
    "# Creación de diccionario con pesos\n",
    "word_weights = {word: abs(weight) for word, weight in top_words}  # abs para que no haya pesos negativos\n",
    "\n",
    "# Generación de nube de palabras\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='tab10').generate_from_frequencies(word_weights)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Palabras que más contribuyen a PCA1\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33741bba-b6af-412e-9698-e4544a2c34cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selección de 3 Componentes Principales\n",
    "\n",
    "# PCA con 3 componentes\n",
    "components = 3\n",
    "pca_3d = PCA(n_components = components)\n",
    "X_pca_3d = pca_3d.fit_transform(X_dev_tfidf.toarray())  # Pasa el array a denso si es sparse\n",
    "\n",
    "# Figura 3D\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for speaker in set(y_dev):\n",
    "    idx = y_dev == speaker\n",
    "    ax.scatter(X_pca_3d[idx, 0], X_pca_3d[idx, 1], X_pca_3d[idx, 2], label=speaker, s=50, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel(\"Componente Principal 1\")\n",
    "ax.set_ylabel(\"Componente Principal 2\")\n",
    "ax.set_zlabel(\"Componente Principal 3\", labelpad=0)\n",
    "ax.set_title(\"Visualización 3D de PCA los discursos (TF-IDF)\")\n",
    "ax.legend(title=\"Candidato:\")\n",
    "plt.tight_layout(pad=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8e7a2-abad-410a-bec8-391c6e9cfd5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Haga una visualización que permita entender cómo varía la varianza explicada a medida que se agregan componentes (e.g: hasta 10 componentes).\n",
    "\n",
    "# PCA sobre la matriz TF-IDF\n",
    "components = 100\n",
    "pca = PCA(n_components = components)\n",
    "X_pca = pca.fit_transform(X_dev_tfidf.toarray())\n",
    "\n",
    "# Varianza explicada individual\n",
    "varianza_explicada = pca.explained_variance_ratio_\n",
    "# Varianza explicada acumulada\n",
    "varianza_acumulada = np.cumsum(varianza_explicada)\n",
    "\n",
    "# Gráfico\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.plot(range(1, components+1), varianza_acumulada, marker='o', linestyle='--', color='b')\n",
    "plt.xticks(range(1, components+1, 3))\n",
    "plt.xlabel('Número de Componentes Principales')\n",
    "plt.ylabel('Varianza Explicada Acumulada')\n",
    "plt.title('Varianza Explicada Acumulada vs. Número de Componentes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0272f140",
   "metadata": {},
   "source": [
    "## Parte 2: Entrenamiento y Evaluación de Modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "0205137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Entrene el modelo Multinomial Naive Bayes, luego utilícelo para predecir sobre el conjunto de test, y reporte el valor de accuracy y la matriz de confusión. Reporte el valor de precision y recall para cada candidato. \n",
    "# Calcular matriz de confusión Sugerencia: utilice el método from_predictions de ConfusionMatrixDisplay para realizar la matriz.\n",
    "\n",
    "# Primero ajustar y transformar los datos de entrenamiento\n",
    "X_dev_bow = vectorizer.fit_transform(X_dev)\n",
    "X_dev_tfidf = tfidf_transformer.fit_transform(X_dev_bow)\n",
    "\n",
    "# Luego transformar los datos de test usando el vectorizador y transformador ya ajustados\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_bow)\n",
    "\n",
    "# Entrenar el modelo Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_dev_tfidf, y_dev)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de test\n",
    "y_pred = mnb.predict(X_test_tfidf)\n",
    "\n",
    "# Calcular accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calcular precision y recall para cada candidato\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Crear un DataFrame para mostrar las métricas por candidato\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Precision': precision,\n",
    "    'Recall': recall\n",
    "}, index=mnb.classes_)\n",
    "print(\"\\nMétricas por candidato:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, \n",
    "    y_pred,\n",
    "    display_labels=mnb.classes_,\n",
    "    cmap='Blues'\n",
    ")\n",
    "plt.title('Matriz de Confusión - Multinomial Naive Bayes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "1b3abeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0acd65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Elija el mejor modelo (mejores parámetros) y vuelva a entrenar sobre todo el conjunto de entrenamiento disponible (sin quitar datos para validación). Reporte el valor final de las métricas y la matriz de confusión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "233006b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Evalúe con validación cruzada al menos un modelo más (dentro de scikit-learn) aparte de Multinomial Naive Bayes para clasificar el texto utilizando las mismas features de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "a25201dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Evalúe el problema cambiando al menos un candidato. En particular, observe el (des)balance de datos y los problemas que pueda generar, así como cualquier indicio que pueda ver en el mapeo previo con PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "addddaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCIONAL: Repetir la clasificación con los tres candidatos con más discursos, pero esta vez clasificando a nivel de párrafos y no de discursos enteros.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
